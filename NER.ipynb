{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMl7OZpiLIqw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57a66707-29c2-4234-fcc8-40b9d4fa3a93"
      },
      "source": [
        "import functools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from evaluation import precision_recall_f1\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaXLavt1yQGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            # Replace all urls with <URL> token\n",
        "            # Replace all users with <USR> token\n",
        "\n",
        "            ######################################\n",
        "            ######### YOUR CODE HERE #############\n",
        "            ######################################\n",
        "            if token.startswith('@'):\n",
        "                token = '<USR>'\n",
        "            if token.startswith('http://') or token.startswith('https://'):\n",
        "                token = '<URL>'\n",
        "            \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKMPMkIRy5uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens, train_tags = read_data('data/train.txt')\n",
        "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
        "test_tokens, test_tags = read_data('data/test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRsDyZNnzgeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25ca830c-0cf0-48cf-d41f-7b8616fe2ce6"
      },
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT\tO\n",
            "<USR>\tO\n",
            ":\tO\n",
            "Online\tO\n",
            "ticket\tO\n",
            "sales\tO\n",
            "for\tO\n",
            "Ghostland\tB-musicartist\n",
            "Observatory\tI-musicartist\n",
            "extended\tO\n",
            "until\tO\n",
            "6\tO\n",
            "PM\tO\n",
            "EST\tO\n",
            "due\tO\n",
            "to\tO\n",
            "high\tO\n",
            "demand\tO\n",
            ".\tO\n",
            "Get\tO\n",
            "them\tO\n",
            "before\tO\n",
            "they\tO\n",
            "sell\tO\n",
            "out\tO\n",
            "...\tO\n",
            "\n",
            "Apple\tB-product\n",
            "MacBook\tI-product\n",
            "Pro\tI-product\n",
            "A1278\tI-product\n",
            "13.3\tI-product\n",
            "\"\tI-product\n",
            "Laptop\tI-product\n",
            "-\tI-product\n",
            "MD101LL/A\tI-product\n",
            "(\tO\n",
            "June\tO\n",
            ",\tO\n",
            "2012\tO\n",
            ")\tO\n",
            "-\tO\n",
            "Full\tO\n",
            "read\tO\n",
            "by\tO\n",
            "eBay\tB-company\n",
            "<URL>\tO\n",
            "<URL>\tO\n",
            "\n",
            "Happy\tO\n",
            "Birthday\tO\n",
            "<USR>\tO\n",
            "!\tO\n",
            "May\tO\n",
            "Allah\tB-person\n",
            "s.w.t\tO\n",
            "bless\tO\n",
            "you\tO\n",
            "with\tO\n",
            "goodness\tO\n",
            "and\tO\n",
            "happiness\tO\n",
            ".\tO\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf2z3__dz4V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    tokens_or_tags = [*functools.reduce(lambda x, y: x + y, tokens_or_tags)]\n",
        "    unique_tokens_or_tags = [x for x in set(tokens_or_tags)\n",
        "                                if x not in special_tokens]\n",
        "    for i, tok in enumerate(special_tokens + unique_tokens_or_tags):\n",
        "        tok2idx[tok] = i\n",
        "        idx2tok.append(tok)\n",
        "    \n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfOzlmK15Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens,\n",
        "                                  special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdQwJ-T12C8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJfiG_gC2pwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
        "        x = np.ones([current_batch_size, max_len_token],\n",
        "                    dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token],\n",
        "                    dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq6MtdYY3CBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMModel():\n",
        "\n",
        "    def __init__(self, vocabulary_size, n_tags, embedding_dim,\n",
        "                 n_hidden_rnn, PAD_index):\n",
        "        self.__declare_placeholders()\n",
        "        self.__build_layers(vocabulary_size, embedding_dim,\n",
        "                            n_hidden_rnn, n_tags)\n",
        "        self.__compute_predictions()\n",
        "        self.__compute_loss(n_tags, PAD_index)\n",
        "        self.__perform_optimization()\n",
        "    \n",
        "    def __declare_placeholders(self):\n",
        "        # Placeholders for input and ground truth output.\n",
        "        self.input_batch = tf.placeholder(dtype=tf.int32,\n",
        "                                          shape=[None, None],\n",
        "                                          name='input_batch') \n",
        "        self.ground_truth_tags = tf.placeholder(dtype=tf.int32,\n",
        "                                                shape=[None, None],\n",
        "                                                name='ground_truth_tags')\n",
        "    \n",
        "        # Placeholder for lengths of the sequences.\n",
        "        self.lengths = tf.placeholder(dtype=tf.int32,\n",
        "                                      shape=[None],\n",
        "                                      name='lengths') \n",
        "        \n",
        "        # Placeholder for a dropout keep probability. If we don't feed\n",
        "        # a value for this placeholder, it will be equal to 1.0.\n",
        "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32),\n",
        "                                                      shape=[])\n",
        "        \n",
        "        # Placeholder for a learning rate.\n",
        "        self.learning_rate_ph = tf.placeholder(dtype=tf.float32,\n",
        "                                               shape=[], \n",
        "                                               name='learning_rate_ph')\n",
        "        \n",
        "    def __build_layers(self, vocabulary_size, embedding_dim,\n",
        "                     n_hidden_rnn, n_tags):\n",
        "        # Create embedding variable\n",
        "        initial_embedding_matrix = np.random.randn(vocabulary_size,\n",
        "                                    embedding_dim) / np.sqrt(embedding_dim)\n",
        "        embedding_matrix_variable = tf.Variable(initial_embedding_matrix,\n",
        "                                                dtype=tf.float32,\n",
        "                                                name='embeddings_matrix')\n",
        "        # Create RNN cells\n",
        "        forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
        "                            tf.nn.rnn_cell.LSTMCell(n_hidden_rnn),\n",
        "                            input_keep_prob=self.dropout_ph,\n",
        "                            output_keep_prob=self.dropout_ph,\n",
        "                            state_keep_prob=self.dropout_ph)\n",
        "        backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
        "                            tf.nn.rnn_cell.LSTMCell(n_hidden_rnn),\n",
        "                            input_keep_prob=self.dropout_ph,\n",
        "                            output_keep_prob=self.dropout_ph,\n",
        "                            state_keep_prob=self.dropout_ph)\n",
        "        \n",
        "        # Shape: [batch_size, sequence_len, embedding_dim].\n",
        "        embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable,\n",
        "                                             self.input_batch)\n",
        "        \n",
        "        # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn].\n",
        "        (rnn_output_fw, rnn_output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "                                        forward_cell,\n",
        "                                        backward_cell,\n",
        "                                        embeddings,\n",
        "                                        sequence_length=self.lengths,\n",
        "                                        dtype=tf.float32)\n",
        "        rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "\n",
        "        self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)\n",
        "\n",
        "    def __compute_predictions(self):\n",
        "        softmax_output = tf.nn.softmax(self.logits)\n",
        "        self.predictions = tf.math.argmax(softmax_output, axis=-1)\n",
        "\n",
        "    def __compute_loss(self, n_tags, PAD_index):\n",
        "        ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "        loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            ground_truth_tags_one_hot, self.logits)\n",
        "        mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
        "        self.loss = tf.reduce_mean(loss_tensor * mask)\n",
        "\n",
        "    def __perform_optimization(self):\n",
        "        self.optimizer =  tf.train.AdamOptimizer()\n",
        "        self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "        \n",
        "        # Gradient clipping only for gradients because compute_gradients method\n",
        "        # also returns variables.\n",
        "        clip_norm = tf.cast(1.0, tf.float32)\n",
        "        self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) \n",
        "                                for grad, var in self.grads_and_vars]\n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
        "\n",
        "    def train_on_batch(self, session, x_batch, y_batch, lengths,\n",
        "                       learning_rate, dropout_keep_probability):\n",
        "        feed_dict = {self.input_batch: x_batch,\n",
        "                     self.ground_truth_tags: y_batch,\n",
        "                     self.learning_rate_ph: learning_rate,\n",
        "                     self.dropout_ph: dropout_keep_probability,\n",
        "                     self.lengths: lengths}\n",
        "        \n",
        "        session.run(self.train_op, feed_dict=feed_dict)\n",
        "\n",
        "    def predict_for_batch(self, session, x_batch, lengths):\n",
        "        feed_dict = {self.input_batch: x_batch,\n",
        "                     self.lengths: lengths}\n",
        "        \n",
        "        predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPEV6GQ2Bc7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_tags(model, session, token_idxs_batch, lengths): \n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session,\n",
        "                                                x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]),\n",
        "                                                        len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0],\n",
        "                                               tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag\n",
        "        # to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True,\n",
        "                                  short_report=short_report)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5dPGde_BuBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx),\n",
        "                    embedding_dim=200, n_hidden_rnn=200,\n",
        "                    PAD_index=token2idx['<PAD>'])\n",
        "\n",
        "batch_size = 32\n",
        "n_epochs = 10\n",
        "learning_rate = 0.05\n",
        "learning_rate_decay = 0.9\n",
        "dropout_keep_probability = 0.6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri8_P0PECUZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d8e9321-3f22-4c81-9a98-a81b3713d0ae"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "    print('=' * 20 + f' Epoch {epoch+1} ' + f'of {n_epochs} ' + '=' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags,\n",
        "               short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, \n",
        "                                                       train_tokens,\n",
        "                                                       train_tags):\n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate,\n",
        "                             dropout_keep_probability)\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training... \n",
            "\n",
            "==================== Epoch 1 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 79204 phrases; correct: 182.\n",
            "\n",
            "precision:  0.23%; recall:  4.05%; F1:  0.43\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 9612 phrases; correct: 28.\n",
            "\n",
            "precision:  0.29%; recall:  5.21%; F1:  0.55\n",
            "\n",
            "==================== Epoch 2 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 16 phrases; correct: 0.\n",
            "\n",
            "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 2 phrases; correct: 0.\n",
            "\n",
            "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
            "\n",
            "==================== Epoch 3 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 3494 phrases; correct: 470.\n",
            "\n",
            "precision:  13.45%; recall:  10.47%; F1:  11.78\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 281 phrases; correct: 34.\n",
            "\n",
            "precision:  12.10%; recall:  6.33%; F1:  8.31\n",
            "\n",
            "==================== Epoch 4 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4966 phrases; correct: 978.\n",
            "\n",
            "precision:  19.69%; recall:  21.79%; F1:  20.69\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 365 phrases; correct: 59.\n",
            "\n",
            "precision:  16.16%; recall:  10.99%; F1:  13.08\n",
            "\n",
            "==================== Epoch 5 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 5666 phrases; correct: 1283.\n",
            "\n",
            "precision:  22.64%; recall:  28.58%; F1:  25.27\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 474 phrases; correct: 94.\n",
            "\n",
            "precision:  19.83%; recall:  17.50%; F1:  18.60\n",
            "\n",
            "==================== Epoch 6 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4743 phrases; correct: 2202.\n",
            "\n",
            "precision:  46.43%; recall:  49.05%; F1:  47.70\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 375 phrases; correct: 133.\n",
            "\n",
            "precision:  35.47%; recall:  24.77%; F1:  29.17\n",
            "\n",
            "==================== Epoch 7 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4976 phrases; correct: 2953.\n",
            "\n",
            "precision:  59.34%; recall:  65.78%; F1:  62.40\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 493 phrases; correct: 163.\n",
            "\n",
            "precision:  33.06%; recall:  30.35%; F1:  31.65\n",
            "\n",
            "==================== Epoch 8 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4754 phrases; correct: 3256.\n",
            "\n",
            "precision:  68.49%; recall:  72.53%; F1:  70.45\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 405 phrases; correct: 170.\n",
            "\n",
            "precision:  41.98%; recall:  31.66%; F1:  36.09\n",
            "\n",
            "==================== Epoch 9 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4720 phrases; correct: 3676.\n",
            "\n",
            "precision:  77.88%; recall:  81.89%; F1:  79.83\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 433 phrases; correct: 181.\n",
            "\n",
            "precision:  41.80%; recall:  33.71%; F1:  37.32\n",
            "\n",
            "==================== Epoch 10 of 10 ====================\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4606 phrases; correct: 3911.\n",
            "\n",
            "precision:  84.91%; recall:  87.12%; F1:  86.00\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 414 phrases; correct: 177.\n",
            "\n",
            "precision:  42.75%; recall:  32.96%; F1:  37.22\n",
            "\n",
            "...training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mht5Ew_1D26F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94a4d98c-e6de-456c-f0ed-9c7a149e640c"
      },
      "source": [
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags,\n",
        "                           short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens,\n",
        "                                validation_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags,\n",
        "                          short_report=False)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "processed 105778 tokens with 4489 phrases; found: 4624 phrases; correct: 4094.\n",
            "\n",
            "precision:  88.54%; recall:  91.20%; F1:  89.85\n",
            "\n",
            "\t     company: precision:   92.25%; recall:   96.27%; F1:   94.22; predicted:   671\n",
            "\n",
            "\t    facility: precision:   90.77%; recall:   93.95%; F1:   92.33; predicted:   325\n",
            "\n",
            "\t     geo-loc: precision:   94.47%; recall:   97.69%; F1:   96.05; predicted:  1030\n",
            "\n",
            "\t       movie: precision:   42.42%; recall:   41.18%; F1:   41.79; predicted:    66\n",
            "\n",
            "\t musicartist: precision:   75.13%; recall:   62.50%; F1:   68.24; predicted:   193\n",
            "\n",
            "\t       other: precision:   90.36%; recall:   91.68%; F1:   91.02; predicted:   768\n",
            "\n",
            "\t      person: precision:   93.22%; recall:   96.28%; F1:   94.73; predicted:   915\n",
            "\n",
            "\t     product: precision:   71.83%; recall:   88.99%; F1:   79.49; predicted:   394\n",
            "\n",
            "\t  sportsteam: precision:   82.13%; recall:   88.94%; F1:   85.40; predicted:   235\n",
            "\n",
            "\t      tvshow: precision:   40.74%; recall:   18.97%; F1:   25.88; predicted:    27\n",
            "\n",
            "-------------------- Validation set quality: --------------------\n",
            "processed 12836 tokens with 537 phrases; found: 432 phrases; correct: 185.\n",
            "\n",
            "precision:  42.82%; recall:  34.45%; F1:  38.18\n",
            "\n",
            "\t     company: precision:   60.64%; recall:   54.81%; F1:   57.58; predicted:    94\n",
            "\n",
            "\t    facility: precision:   35.90%; recall:   41.18%; F1:   38.36; predicted:    39\n",
            "\n",
            "\t     geo-loc: precision:   66.67%; recall:   47.79%; F1:   55.67; predicted:    81\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     4\n",
            "\n",
            "\t musicartist: precision:   20.00%; recall:    3.57%; F1:    6.06; predicted:     5\n",
            "\n",
            "\t       other: precision:   31.43%; recall:   27.16%; F1:   29.14; predicted:    70\n",
            "\n",
            "\t      person: precision:   43.84%; recall:   28.57%; F1:   34.59; predicted:    73\n",
            "\n",
            "\t     product: precision:    8.57%; recall:    8.82%; F1:    8.70; predicted:    35\n",
            "\n",
            "\t  sportsteam: precision:    6.45%; recall:   10.00%; F1:    7.84; predicted:    31\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "-------------------- Test set quality: --------------------\n",
            "processed 13258 tokens with 604 phrases; found: 433 phrases; correct: 223.\n",
            "\n",
            "precision:  51.50%; recall:  36.92%; F1:  43.01\n",
            "\n",
            "\t     company: precision:   59.65%; recall:   40.48%; F1:   48.23; predicted:    57\n",
            "\n",
            "\t    facility: precision:   43.48%; recall:   42.55%; F1:   43.01; predicted:    46\n",
            "\n",
            "\t     geo-loc: precision:   79.09%; recall:   52.73%; F1:   63.27; predicted:   110\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "\t musicartist: precision:   33.33%; recall:    3.70%; F1:    6.67; predicted:     3\n",
            "\n",
            "\t       other: precision:   33.33%; recall:   31.07%; F1:   32.16; predicted:    96\n",
            "\n",
            "\t      person: precision:   56.72%; recall:   36.54%; F1:   44.44; predicted:    67\n",
            "\n",
            "\t     product: precision:   13.64%; recall:   10.71%; F1:   12.00; predicted:    22\n",
            "\n",
            "\t  sportsteam: precision:   26.67%; recall:   25.81%; F1:   26.23; predicted:    30\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}